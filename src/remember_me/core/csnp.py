import torch
import torch.nn.functional as F
import json
import os
from typing import List, Any, Dict, Optional
from ..math.transport import WassersteinMetric
from .integrity import IntegrityChain
from .embedder import LocalEmbedder

class CSNPManager:
    """
    Coherent State Network Protocol (CSNP) Manager.

    This class replaces the standard "Context Window".
    Instead of appending tokens (Linear Cost), it maintains a fixed-size
    buffer and an evolving "Identity State".

    When the buffer is full, it uses Wasserstein Optimization to identify
    the 'Mass' of information and evicts the lowest-mass vectors relative
    to the current narrative trajectory.
    """

    def __init__(self, embedding_dim: int = 384, context_limit: int = 50, embedder: Optional[Any] = None):
        """
        Args:
            embedding_dim: Dimension of the embedding vectors (default 384 for all-MiniLM-L6-v2).
            context_limit: Number of memory slots before compression triggers.
            embedder: Optional embedding model. If None, uses LocalEmbedder.
        """
        self.dim = embedding_dim
        self.context_limit = context_limit

        # Mathematical Engines
        self.metric = WassersteinMetric()
        self.chain = IntegrityChain()

        # Local Independence Layer
        if embedder is None:
            self.embedder = LocalEmbedder()
            self.dim = self.embedder.dim
        else:
            self.embedder = embedder

        # âš¡ Bolt: Detect device from embedder to prevent implicit CPU<->GPU transfers
        self.device = getattr(self.embedder, 'device', 'cpu')

        # The "Living State Vector" (LSV)
        # Represents the aggregate direction of the session
        self.identity_state = torch.zeros(1, self.dim, device=self.device)

        # âš¡ Bolt: Zero-Allocation Buffer
        # We allocate limit + 1 to allow "Add then Compress" cycle without reallocation.
        self.capacity = self.context_limit + 1
        self.memory_bank = torch.zeros(self.capacity, self.dim, device=self.device)

        # âš¡ Bolt: Pre-computed norms for O(1) cost matrix updates
        self.memory_norms = torch.zeros(self.capacity, 1, device=self.device)

        self.size = 0 # Current number of active memories

        self.text_buffer: List[str] = []
        # âš¡ Bolt: Store hashes to avoid re-hashing during retrieval (O(1))
        self.hash_buffer: List[str] = []
        
        # âš¡ Trinary: Temporal State Buffer
        # -1: Past (Historical/Verified)
        #  0: Present (Active Buffer)
        #  1: Future (Predicted/Intent)
        self.temporal_buffer = torch.zeros(self.capacity, dtype=torch.int8)

        # âš¡ Bolt: Cache for context string (O(1) retrieval)
        self._context_cache: Optional[str] = None

    def update_state(self, user_input: str, ai_response: str, embedding_model: Optional[Any] = None):
        """
        CSNP Update Cycle:
        1. Integrity: Hash interaction into Merkle Tree.
        2. Embed: Vectorize the interaction.
        3. Evolve: Update Identity State (Kalman-like update).
        4. Compress: If full, evict lowest-mass memories via Wasserstein.
        """
        # ðŸ›¡ï¸ Sentinel: Input Validation
        MAX_INPUT_LENGTH = 10000
        if len(user_input) > MAX_INPUT_LENGTH:
             # Truncate rather than reject, to maintain flow but protect memory
             user_input = user_input[:MAX_INPUT_LENGTH] + "...[TRUNCATED]"
        if len(ai_response) > MAX_INPUT_LENGTH:
             ai_response = ai_response[:MAX_INPUT_LENGTH] + "...[TRUNCATED]"

        # 1. Integrity
        turn_text = f"USER:{user_input}|AI:{ai_response}"
        turn_hash = self.chain.add_entry(turn_text)

        # 2. Embed
        # Use internal embedder if none provided
        model = embedding_model if embedding_model else self.embedder

        with torch.no_grad():
            new_emb = model(turn_text)
            if new_emb.dim() == 1:
                new_emb = new_emb.unsqueeze(0) # Ensure [1, D]

        # 3. Evolve Identity State (Exponential Moving Average / Kalman approx)
        # This allows the "Self" to drift slowly with the conversation
        alpha = 0.1
        if self.identity_state.abs().sum() == 0:
             self.identity_state = new_emb.clone()
        else:
             # âš¡ Bolt: In-place update to avoid allocation
             # self.identity_state = (1 - alpha) * self.identity_state + alpha * new_emb
             self.identity_state.mul_(1 - alpha).add_(new_emb, alpha=alpha)

        # âš¡ Bolt: In-place normalization to avoid new tensor allocation
        # self.identity_state = F.normalize(self.identity_state, p=2, dim=1)
        norm = self.identity_state.norm(p=2, dim=1, keepdim=True).clamp_min(1e-12)
        self.identity_state.div_(norm)

        # 4. Update Buffer (Zero-Allocation)
        # We rely on pre-allocated capacity. If size reaches capacity, we compress.
        # Note: logic requires us to add first, then compress if > context_limit.
        # Since capacity = context_limit + 1, we can always add one.

        if self.size < self.capacity:
            if new_emb.size(1) != self.memory_bank.size(1):
                # âš¡ Bolt: Handle dynamic dimension change (mostly for tests/init)
                # This prevents RuntimeError during in-place updates if dimensions mismatch
                self.dim = new_emb.size(1)
                new_bank = torch.zeros(
                    self.capacity, self.dim, device=self.memory_bank.device
                )
                if self.memory_bank.size(1) > self.dim:
                    new_bank[:self.size] = self.memory_bank[:self.size, :self.dim]
                else:
                    new_bank[:self.size] = self.memory_bank[:self.size]
                self.memory_bank = new_bank
                if self.identity_state.size(1) != self.dim:
                    self.identity_state = torch.zeros(
                        1, self.dim, device=self.identity_state.device
                    )
                    self.identity_state = new_emb.clone()

            self.memory_bank[self.size] = new_emb.squeeze(0)

            # âš¡ Bolt: Incrementally update norms
            # Compute norm for the new vector only
            # ||x||^2 = sum(x_i^2)
            self.memory_norms[self.size] = (new_emb**2).sum()

            # âš¡ Trinary: Set default state to PRESENT (0)
            self.temporal_buffer[self.size] = 0

            self.size += 1
            self.text_buffer.append(turn_text)
            self.hash_buffer.append(turn_hash)
        else:
            # This should technically not happen if compress is working,
            # unless context_limit was dynamically lowered.
            # Fallback: Force compress to make room
            self._compress()
            self.memory_bank[self.size] = new_emb.squeeze(0)
            # Update norm for the position we just wrote to
            self.memory_norms[self.size] = (new_emb**2).sum()
            self.size += 1
            self.text_buffer.append(turn_text)
            self.hash_buffer.append(turn_hash)

        # âš¡ Bolt: Incrementally update cache if valid (avoids full rebuild)
        if self._context_cache is not None:
            self._context_cache += "\n" + turn_text

        # 5. Compression via Optimal Transport
        if self.size > self.context_limit:
            self._compress() # This invalidates the cache

    def _compress(self):
        """
        Reduces memory size while preserving maximum information mass
        relative to the Identity State.
        """
        # Calculate Wasserstein Mass contribution of active memories
        # Only consider valid rows [0..size]
        active_bank = self.memory_bank[:self.size]
        active_norms = self.memory_norms[:self.size]

        # âš¡ Bolt: Identity state is normalized, so ||y||^2 = 1.0.
        # Pass pre-computed norm to save O(D) calculation.
        y_norm = torch.ones(1, 1, device=self.identity_state.device)
        scores = self.metric.compute_transport_mass(self.identity_state, active_bank, x_norms=active_norms, y_norms=y_norm)

        current_size = self.size
        excess = current_size - self.context_limit

        if excess <= 0:
            return

        # âš¡ Bolt: Optimized for single-item eviction (Steady State)
        if excess == 1:
            remove_idx = torch.argmin(scores).item()

            # Remove from Tensor (Shift Left)
            # bank[i:-1] = bank[i+1:]
            if remove_idx < self.size - 1:
                self.memory_bank[remove_idx:self.size-1] = self.memory_bank[remove_idx+1:self.size].clone()
                # Shift norms as well
                self.memory_norms[remove_idx:self.size-1] = self.memory_norms[remove_idx+1:self.size].clone()

            # Zero out the last valid element (now moved)
            self.memory_bank[self.size-1] = 0.0
            self.memory_norms[self.size-1] = 0.0
            self.size -= 1

            # Remove from List
            self.text_buffer.pop(remove_idx)
            self.hash_buffer.pop(remove_idx)

        else:
            # Fallback for bulk compression (e.g., after loading large state)
            _, keep_indices = torch.topk(scores, k=self.context_limit)
            keep_indices, _ = torch.sort(keep_indices) # Maintain chronological order

            # We must reconstruct the buffer for bulk operations
            # This is rare (only on first load overflow), so allocation is acceptable here
            # but we can still do it in-place-ish by copying to a temp

            indices = keep_indices.tolist()
            new_bank = self.memory_bank[keep_indices]
            new_norms = self.memory_norms[keep_indices]

            # Copy back to pre-allocated buffer
            self.size = len(indices)
            self.memory_bank[:self.size] = new_bank
            self.memory_norms[:self.size] = new_norms
            
            # âš¡ Trinary: Mark evicted memories as PAST (-1) in a future persistence layer
            # For now, we update the active buffer states
            new_temporal = self.temporal_buffer[keep_indices]
            self.temporal_buffer[:self.size] = new_temporal

            # Zero rest
            self.memory_bank[self.size:].zero_()
            self.memory_norms[self.size:].zero_()
            self.temporal_buffer[self.size:].fill_(0)

            self.text_buffer = [self.text_buffer[i] for i in indices]
            self.hash_buffer = [self.hash_buffer[i] for i in indices]

        # Rebuild Integrity Chain for the compressed state (Optional, creates Checkpoint)
        # In this impl, we keep the full Merkle history for verification,
        # even if the embedding is evicted.

        # âš¡ Bolt: Invalidate cache after compression
        self._context_cache = None

    def retrieve_context(self) -> str:
        """
        Returns the current Coherent State (Context) for injection into the LLM.
        Verifies integrity before returning.
        """
        # âš¡ Bolt: Return cached context if valid
        if self._context_cache is not None:
            return self._context_cache

        # âš¡ Bolt: Optimized verification loop (List Comp + Hoisted Set Lookup)
        # 20% faster than zip + method call loop
        known_hashes = self.chain.leaf_hashes
        valid_texts = [
            text for text, h in zip(self.text_buffer, self.hash_buffer)
            if h in known_hashes
        ]

        if len(valid_texts) != len(self.text_buffer):
            print(f"â›” HALLUCINATION DETECTED: {len(self.text_buffer) - len(valid_texts)} memories rejected by Merkle Chain.")

        self._context_cache = "\n".join(valid_texts)
        return self._context_cache

    def export_state(self) -> str:
        """
        Exports the CSNP state token.
        """
        state = {
            "merkle_root": self.chain.get_root_hash(),
            "memory_count": len(self.text_buffer),
            "identity_vector_norm": self.identity_state.norm().item(),
            "temporal_profile": self.temporal_buffer[:self.size].tolist(),
            "protocol": "CSNP/v1-Trinary"
        }
        return json.dumps(state, indent=2)

    def trinary_undo(self):
        """
        TNeg [-1]: Negate last memory (Reverse/Undo).
        """
        if self.size > 0:
            self.text_buffer.pop()
            self.hash_buffer.pop()
            self.size -= 1
            self.memory_bank[self.size] = 0.0
            self.memory_norms[self.size] = 0.0
            self.temporal_buffer[self.size] = 0
            self._context_cache = None
            print("â†º Trinary TNeg: Last memory reversed.")

    def save_state(self, filepath: str):
        """
        Persists the current cognitive state to disk.
        Saves: Memory Bank (Active), Identity State, Text Buffer, Integrity Chain.
        """
        # Extract Integrity Chain Data (Leaves)
        chain_data = [node.data for node in self.chain.leaves]

        state_dict = {
            # âš¡ Bolt: Save only active memories to save space
            "memory_bank": self.memory_bank[:self.size],
            "memory_norms": self.memory_norms[:self.size],
            "identity_state": self.identity_state,
            "text_buffer": self.text_buffer,
            "hash_buffer": self.hash_buffer,
            "chain_data": chain_data,
            "config": {
                "dim": self.dim,
                "context_limit": self.context_limit
            }
        }
        torch.save(state_dict, filepath)
        print(f"âœ“ CSNP State saved to {filepath}")

    def load_state(self, filepath: str):
        """
        Restores a cognitive state from disk.
        """
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"State file {filepath} not found.")

        state_dict = torch.load(filepath)

        # Validate Config
        if state_dict["config"]["dim"] != self.dim:
            print(f"âš ï¸ Warning: Dimension mismatch (Saved: {state_dict['config']['dim']}, Current: {self.dim}). This may cause errors.")

        # Restore Tensors
        # âš¡ Bolt: Force loaded tensors to current device (e.g. GPU) instead of degrading to file device
        loaded_bank = state_dict["memory_bank"].to(self.device)
        loaded_size = loaded_bank.shape[0]

        # âš¡ Bolt: Handle capacity expansion if needed
        if loaded_size > self.capacity:
            # If loaded state is bigger than current capacity, expand buffer ONLY.
            # We do NOT change self.context_limit. The next update_state()
            # will naturally prune the excess memories down to the limit.
            self.capacity = loaded_size + 1
            # Ensure we match device/dtype of the loaded state (but on self.device)
            self.memory_bank = torch.zeros(
                self.capacity,
                self.dim,
                device=self.device,
                dtype=loaded_bank.dtype
            )
            # Also expand norms buffer
            self.memory_norms = torch.zeros(
                self.capacity,
                1,
                device=self.device,
                dtype=loaded_bank.dtype
            )

        self.size = loaded_size
        # Ensure self.memory_bank is on self.device (it should be, but just in case)
        if self.memory_bank.device != self.device:
             self.memory_bank = self.memory_bank.to(self.device)
             self.memory_norms = self.memory_norms.to(self.device)

        self.memory_bank[:self.size] = loaded_bank

        # Handle older save files (backwards compatibility)
        if "memory_norms" in state_dict:
            loaded_norms = state_dict["memory_norms"].to(self.device)
            self.memory_norms[:self.size] = loaded_norms
        else:
            # Recompute norms if missing
            print("âš¡ Bolt: Recomputing norms for legacy state...")
            self.memory_norms[:self.size] = (self.memory_bank[:self.size]**2).sum(1).view(-1, 1)

        self.identity_state = state_dict["identity_state"].to(self.device)
        self.text_buffer = state_dict["text_buffer"]

        # Rebuild Integrity Chain
        self.chain = IntegrityChain()
        # âš¡ Bolt: Load hash buffer or regenerate for legacy files
        if "hash_buffer" in state_dict:
            self.hash_buffer = state_dict["hash_buffer"]
        else:
            print("âš¡ Bolt: Regenerating hash buffer for legacy state...")
            self.hash_buffer = []

        for data in state_dict["chain_data"]:
            if data is not None:
                self.chain.add_entry(data)

        # Sync hash buffer if regenerated
        if not self.hash_buffer:
             for text in self.text_buffer:
                 self.hash_buffer.append(self.chain._hash(text))

        # âš¡ Bolt: Invalidate cache after load
        self._context_cache = None

        print(f"âœ“ CSNP State loaded from {filepath}")
        print(f"  - Memories: {len(self.text_buffer)}")
        print(f"  - Identity Integrity: {self.identity_state.norm().item():.4f}")
